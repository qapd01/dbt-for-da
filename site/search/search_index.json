{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dbt for data engineer Hello","title":"Home"},{"location":"#dbt-for-data-engineer","text":"","title":"Dbt for data engineer"},{"location":"dbt/advance_technique/","text":"Module 4: dbt Best Practices and Advanced Techniques DRY (Don't Repeat Yourself) principles Jinja templating in dbt Macros and reusable code Package management Understanding and creating ref() and source() functions Complex model dependencies Advance Technique What is Jinja in dbt Jinja is a templating engine used by dbt to dynamically generate SQL queries. It allows you to write code that is modular, reusable, and flexible by embedding Python-like expressions and variables into your SQL code. Jinja is built into dbt, so you can use it to customize queries based on parameters, environment variables, or metadata from your project. Basics of Jinja in dbt Expressions Jinja expressions are placed inside {{ ... }} These expressions are evaluated when dbt render the query. SELECT * FROM {{ ref('my_table') }} Control Structures Use Jinja\u2019s {% ... %} for control logic like loops and conditionals. {% if target.name == 'prod' %} SELECT * FROM production_table {% else %} SELECT * FROM staging_table {% endif %} Variables Define variables in dbt_project.yml or pass them at runtime. Access variables in your SQL files using {{ var('variable_name') }} vars: latest_date: 2024-11-04 SELECT * FROM ... WHERE processing_date = DATE(\" {{var('latest_date')}} \") Macro A marco is a reusable block of jinja code. It is defined in .sql files in the macros/ folder. marcos/cents_to_dollars.sql {% macro cents_to_dollars(column_name, scale=2) %} ({{ column_name }} / 100)::numeric(16, {{ scale }}) {% endmacro %} A model which uses this macro might look like: select id as payment_id, {{ cents_to_dollars('amount') }} as amount_usd, ... from app_data.payments This would be compiled to: select id as payment_id, (amount / 100)::numeric(16, 2) as amount_usd, ... from app_data.payments","title":"Advance Techniques"},{"location":"dbt/advance_technique/#advance-technique","text":"","title":"Advance Technique"},{"location":"dbt/advance_technique/#what-is-jinja-in-dbt","text":"Jinja is a templating engine used by dbt to dynamically generate SQL queries. It allows you to write code that is modular, reusable, and flexible by embedding Python-like expressions and variables into your SQL code. Jinja is built into dbt, so you can use it to customize queries based on parameters, environment variables, or metadata from your project. Basics of Jinja in dbt Expressions Jinja expressions are placed inside {{ ... }} These expressions are evaluated when dbt render the query. SELECT * FROM {{ ref('my_table') }} Control Structures Use Jinja\u2019s {% ... %} for control logic like loops and conditionals. {% if target.name == 'prod' %} SELECT * FROM production_table {% else %} SELECT * FROM staging_table {% endif %} Variables Define variables in dbt_project.yml or pass them at runtime. Access variables in your SQL files using {{ var('variable_name') }} vars: latest_date: 2024-11-04 SELECT * FROM ... WHERE processing_date = DATE(\" {{var('latest_date')}} \") Macro A marco is a reusable block of jinja code. It is defined in .sql files in the macros/ folder. marcos/cents_to_dollars.sql {% macro cents_to_dollars(column_name, scale=2) %} ({{ column_name }} / 100)::numeric(16, {{ scale }}) {% endmacro %} A model which uses this macro might look like: select id as payment_id, {{ cents_to_dollars('amount') }} as amount_usd, ... from app_data.payments This would be compiled to: select id as payment_id, (amount / 100)::numeric(16, 2) as amount_usd, ... from app_data.payments","title":"What is Jinja in dbt"},{"location":"dbt/basic_and_project_structure/","text":"Project Structure Structure our dbt projects dbt Basics and Project Structure Understanding dbt project structure Creating your first dbt project Basic project configuration Understanding dbt models Writing SQL transformations Best practices for model organization Intro to dbt Core concepts: Models Sources Seeds Tests jaffle_shop \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 employees.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 cents_to_dollars.sql \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 intermediate \u2502 \u2502 \u2514\u2500\u2500 finance \u2502 \u2502 \u251c\u2500\u2500 _int_finance__models.yml \u2502 \u2502 \u2514\u2500\u2500 int_payments_pivoted_to_orders.sql \u2502 \u251c\u2500\u2500 marts \u2502 \u2502 \u251c\u2500\u2500 finance \u2502 \u2502 \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2502 \u2514\u2500\u2500 marketing \u2502 \u2502 \u251c\u2500\u2500 _marketing__models.yml \u2502 \u2502 \u2514\u2500\u2500 customers.sql \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u251c\u2500\u2500 jaffle_shop \u2502 \u2502 \u2502 \u251c\u2500\u2500 _jaffle_shop__docs.md \u2502 \u2502 \u2502 \u251c\u2500\u2500 _jaffle_shop__models.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 _jaffle_shop__sources.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 base_jaffle_shop__customers.sql \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 base_jaffle_shop__deleted_customers.sql \u2502 \u2502 \u2502 \u251c\u2500\u2500 stg_jaffle_shop__customers.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 stg_jaffle_shop__orders.sql \u2502 \u2502 \u2514\u2500\u2500 stripe \u2502 \u2502 \u251c\u2500\u2500 _stripe__models.yml \u2502 \u2502 \u251c\u2500\u2500 _stripe__sources.yml \u2502 \u2502 \u2514\u2500\u2500 stg_stripe__payments.sql \u2502 \u2514\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql Guide structure overview We'll walk through our topics in the same order that our data would move through transformation: Dig into how we structure the files, folders, and models for our three primary layers in the models directory, which build on each other: Staging \u2014 creating our atoms, our initial modular building blocks, from source data Intermediate \u2014 stacking layers of logic with clear and specific purposes to prepare our staging models to join into the entities we want Marts \u2014 bringing together our modular pieces into a wide, rich vision of the entities our organization cares about Explore how these layers fit into the rest of the project: Review the overall structure comprehensively Expand on YAML configuration in-depth Discuss how to use the other folders in a dbt project: tests, seeds, and analyses","title":"Project Structure"},{"location":"dbt/basic_and_project_structure/#project-structure","text":"","title":"Project Structure"},{"location":"dbt/basic_and_project_structure/#structure-our-dbt-projects","text":"dbt Basics and Project Structure Understanding dbt project structure Creating your first dbt project Basic project configuration Understanding dbt models Writing SQL transformations Best practices for model organization Intro to dbt Core concepts: Models Sources Seeds Tests jaffle_shop \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 employees.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 cents_to_dollars.sql \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 intermediate \u2502 \u2502 \u2514\u2500\u2500 finance \u2502 \u2502 \u251c\u2500\u2500 _int_finance__models.yml \u2502 \u2502 \u2514\u2500\u2500 int_payments_pivoted_to_orders.sql \u2502 \u251c\u2500\u2500 marts \u2502 \u2502 \u251c\u2500\u2500 finance \u2502 \u2502 \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2502 \u2514\u2500\u2500 marketing \u2502 \u2502 \u251c\u2500\u2500 _marketing__models.yml \u2502 \u2502 \u2514\u2500\u2500 customers.sql \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u251c\u2500\u2500 jaffle_shop \u2502 \u2502 \u2502 \u251c\u2500\u2500 _jaffle_shop__docs.md \u2502 \u2502 \u2502 \u251c\u2500\u2500 _jaffle_shop__models.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 _jaffle_shop__sources.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 base_jaffle_shop__customers.sql \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 base_jaffle_shop__deleted_customers.sql \u2502 \u2502 \u2502 \u251c\u2500\u2500 stg_jaffle_shop__customers.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 stg_jaffle_shop__orders.sql \u2502 \u2502 \u2514\u2500\u2500 stripe \u2502 \u2502 \u251c\u2500\u2500 _stripe__models.yml \u2502 \u2502 \u251c\u2500\u2500 _stripe__sources.yml \u2502 \u2502 \u2514\u2500\u2500 stg_stripe__payments.sql \u2502 \u2514\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql Guide structure overview We'll walk through our topics in the same order that our data would move through transformation: Dig into how we structure the files, folders, and models for our three primary layers in the models directory, which build on each other: Staging \u2014 creating our atoms, our initial modular building blocks, from source data Intermediate \u2014 stacking layers of logic with clear and specific purposes to prepare our staging models to join into the entities we want Marts \u2014 bringing together our modular pieces into a wide, rich vision of the entities our organization cares about Explore how these layers fit into the rest of the project: Review the overall structure comprehensively Expand on YAML configuration in-depth Discuss how to use the other folders in a dbt project: tests, seeds, and analyses","title":"Structure our dbt projects"},{"location":"dbt/dbt_cmd/","text":"Basic command for dbt","title":"Basic command for dbt"},{"location":"dbt/dbt_cmd/#basic-command-for-dbt","text":"","title":"Basic command for dbt"},{"location":"dbt/dbt_data_quality_and_testing/","text":"Module 5: Data Quality and Testing Introduction to dbt testing Writing custom data quality tests Generic vs. singular tests Implementing data validation Monitoring and alerting strategies Best practices for ensuring data reliability DBT Testing: Ensuring Data Quality and Reliability Data tests Types of Tests in dbt Generic Tests Built-in tests that can be applied across different models: models: - name: orders columns: - name: order_id tests: - unique # Ensures column values are unique - not_null # Ensures no null values - name: amount tests: - accepted_values: values: ['>0'] # Custom value validation - name: customer_id tests: - relationships: to: ref('customers') # Referential integrity test field: customer_id Unit tests Model with customers as ( select * from {{ ref('stg_customers') }} ), accepted_email_domains as ( select * from {{ ref('top_level_email_domains') }} ), check_valid_emails as ( select customers.customer_id, customers.first_name, customers.last_name, customers.email, coalesce (regexp_like( customers.email, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$' ) = true and accepted_email_domains.tld is not null, false) as is_valid_email_address from customers left join accepted_email_domains on customers.email_top_level_domain = lower(accepted_email_domains.tld) ) select * from check_valid_emails Unit test file unit test place in src tests/case_1_..._.md unit_tests: - name: test_is_valid_email_address description: \"Check my is_valid_email_address logic captures all known edge cases - emails without ., emails without @, and emails from invalid domains.\" model: dim_customers given: - input: ref('stg_customers') rows: - {email: cool@example.com, email_top_level_domain: example.com} - {email: cool@unknown.com, email_top_level_domain: unknown.com} - {email: badgmail.com, email_top_level_domain: gmail.com} - {email: missingdot@gmailcom, email_top_level_domain: gmail.com} - input: ref('top_level_email_domains') rows: - {tld: example.com} - {tld: gmail.com} expect: rows: - {email: cool@example.com, is_valid_email_address: true} - {email: cool@unknown.com, is_valid_email_address: false} - {email: badgmail.com, is_valid_email_address: false} - {email: missingdot@gmailcom, is_valid_email_address: false} Run the dbt test command: $ dbt test Found 3 models, 2 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 0 seed files, 0 sources 17:31:05 | Concurrency: 1 threads (target='learn') 17:31:05 | 17:31:05 | 1 of 2 START test not_null_order_order_id..................... [RUN] 17:31:06 | 1 of 2 PASS not_null_order_order_id........................... [PASS in 0.99s] 17:31:06 | 2 of 2 START test unique_order_order_id....................... [RUN] 17:31:07 | 2 of 2 PASS unique_order_order_id............................. [PASS in 0.79s] 17:31:07 | 17:31:07 | Finished running 2 tests in 7.17s. Completed successfully Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2","title":"Data Quality and Testing"},{"location":"dbt/dbt_data_quality_and_testing/#dbt-testing-ensuring-data-quality-and-reliability","text":"","title":"DBT Testing: Ensuring Data Quality and Reliability"},{"location":"dbt/dbt_data_quality_and_testing/#data-tests","text":"Types of Tests in dbt Generic Tests Built-in tests that can be applied across different models: models: - name: orders columns: - name: order_id tests: - unique # Ensures column values are unique - not_null # Ensures no null values - name: amount tests: - accepted_values: values: ['>0'] # Custom value validation - name: customer_id tests: - relationships: to: ref('customers') # Referential integrity test field: customer_id","title":"Data tests"},{"location":"dbt/dbt_data_quality_and_testing/#unit-tests","text":"Model with customers as ( select * from {{ ref('stg_customers') }} ), accepted_email_domains as ( select * from {{ ref('top_level_email_domains') }} ), check_valid_emails as ( select customers.customer_id, customers.first_name, customers.last_name, customers.email, coalesce (regexp_like( customers.email, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$' ) = true and accepted_email_domains.tld is not null, false) as is_valid_email_address from customers left join accepted_email_domains on customers.email_top_level_domain = lower(accepted_email_domains.tld) ) select * from check_valid_emails Unit test file unit test place in src tests/case_1_..._.md unit_tests: - name: test_is_valid_email_address description: \"Check my is_valid_email_address logic captures all known edge cases - emails without ., emails without @, and emails from invalid domains.\" model: dim_customers given: - input: ref('stg_customers') rows: - {email: cool@example.com, email_top_level_domain: example.com} - {email: cool@unknown.com, email_top_level_domain: unknown.com} - {email: badgmail.com, email_top_level_domain: gmail.com} - {email: missingdot@gmailcom, email_top_level_domain: gmail.com} - input: ref('top_level_email_domains') rows: - {tld: example.com} - {tld: gmail.com} expect: rows: - {email: cool@example.com, is_valid_email_address: true} - {email: cool@unknown.com, is_valid_email_address: false} - {email: badgmail.com, is_valid_email_address: false} - {email: missingdot@gmailcom, is_valid_email_address: false} Run the dbt test command: $ dbt test Found 3 models, 2 tests, 0 snapshots, 0 analyses, 130 macros, 0 operations, 0 seed files, 0 sources 17:31:05 | Concurrency: 1 threads (target='learn') 17:31:05 | 17:31:05 | 1 of 2 START test not_null_order_order_id..................... [RUN] 17:31:06 | 1 of 2 PASS not_null_order_order_id........................... [PASS in 0.99s] 17:31:06 | 2 of 2 START test unique_order_order_id....................... [RUN] 17:31:07 | 2 of 2 PASS unique_order_order_id............................. [PASS in 0.79s] 17:31:07 | 17:31:07 | Finished running 2 tests in 7.17s. Completed successfully Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2","title":"Unit tests"},{"location":"dbt/dbt_docs/","text":"DBT DOC Good documentation for your dbt models will help downstream consumers discover and understand the datasets you curate for them. dbt provides a way to generate documentation for your dbt project and render it as a website. dbt docs generate","title":"Documentation"},{"location":"dbt/dbt_docs/#dbt-doc","text":"Good documentation for your dbt models will help downstream consumers discover and understand the datasets you curate for them. dbt provides a way to generate documentation for your dbt project and render it as a website. dbt docs generate","title":"DBT DOC"},{"location":"dbt/sql_transformation_and_modeling/","text":"Module 3: SQL Transformations and Modeling Writing effective dbt models Understanding materialization types Tables Views Incremental models Handling complex transformations Writing clean, modular SQL Performance optimization techniques Working with common data transformation patterns 1 Materialization Types Materialization defines how your models are created and stored in the data warehouse: Table Materialization Creates a full table in the database Best for complex transformations or when you need fast query performance Rebuilds the entire table each time {{ config(materialized='table') }} SELECT customer_id, SUM(order_amount) as total_revenue, COUNT(DISTINCT order_id) as order_count FROM {{ ref('raw_orders') }} GROUP BY customer_id View Materialization Creates a virtual table (saved query) Lightweight, always reflects latest source data Good for simple transformations {{ config(materialized='view') }} SELECT date, SUM(sales_amount) as daily_sales FROM {{ ref('sales_data') }} GROUP BY date Incremental Materialization Only processes new or updated records Ideal for large datasets and frequent updates {{ config( materialized='incremental', unique_key='order_id' ) }} SELECT order_id, customer_id, order_date, total_amount FROM {{ ref('raw_orders') }} {% if is_incremental() %} WHERE order_date > (SELECT MAX(order_date) FROM {{ this }}) {% endif %} 2 Writing Clean, Modular SQL Modularization Techniques Use ref() to reference other models Break complex transformations into multiple models Create reusable CTEs WITH customer_orders AS ( SELECT customer_id, COUNT(order_id) as total_orders FROM {{ ref('orders') }} GROUP BY customer_id ), customer_revenue AS ( SELECT customer_id, SUM(order_total) as lifetime_revenue FROM {{ ref('orders') }} GROUP BY customer_id ) SELECT c.customer_id, c.total_orders, r.lifetime_revenue FROM customer_orders c JOIN customer_revenue r ON c.customer_id = r.customer_id 3 Performance Optimization Techniques Use incremental models for large datasets Partition large tables Use appropriate clustering keys Avoid unnecessary JOINs and complex subqueries {{ config( materialized='incremental', partition_by='order_date', cluster_by=['customer_segment'] ) }} SELECT order_date, customer_segment, SUM(order_total) as segment_revenue FROM {{ ref('orders') }} {% if is_incremental() %} WHERE order_date > (SELECT MAX(order_date) FROM {{ this }}) {% endif %} GROUP BY order_date, customer_segment 4 Common Data Transformation Patterns Slowly Changing Dimensions (SCD) Track historical changes in dimension tables WITH current_data AS ( SELECT customer_id, name, email, CURRENT_TIMESTAMP as valid_from FROM {{ ref('customer_source') }} ), historical_data AS ( SELECT *, ROW_NUMBER() OVER ( PARTITION BY customer_id ORDER BY valid_from DESC ) as rn FROM {{ this }} ) SELECT * FROM ( SELECT customer_id, name, email, valid_from FROM current_data UNION ALL SELECT customer_id, name, email, valid_from FROM historical_data WHERE rn > 1 ) Key Takeaways for dbt Beginners: Start with simple models Use ref() to create dependencies Choose the right materialization type Keep transformations modular Focus on readability and performance Pro Tips: Use dbt's built-in testing Leverage dbt documentation Learn to use dbt Cloud or dbt Core Practice with sample datasets Remember, dbt is about making your data transformations more maintainable, transparent, and efficient!","title":"Sql transformation and modeling"},{"location":"dbt/sql_transformation_and_modeling_1/","text":"Materializations Overview Materializations are strategies for persisting dbt models in a warehouse. There are five types of materializations built into dbt. They are: Table View Incremental Ephemeral Materialized View Table Pros: Table are fast to query. Cons: Tables can take a long time to rebuild, especially for complex transformations New records in underlying source data are not automatically added to the table View When using the view materialization, your model is rebuilt as a view on each run, via a create view as statement. Pros: No additional data is stored, views on top of source data will always have the latest records in them. Cons: Views that perform a significant transformation, or are stacked on top of other views, are slow to query. Incremental Pros: You can significantly reduce the build time by just transforming new records Cons: Incremental models require extra configuration and are an advanced usage of dbt Ephemeral Pros: You can still write reusable logic Ephemeral models can help keep your data warehouse clean by reducing clutter (also consider splitting your models across multiple schemas by using custom schemas). Cons: You cannot select directly from this model. Operations (for example, macros called using dbt run-operation cannot ref() ephemeral nodes) Overuse of ephemeral materialization can also make queries harder to debug. Ephemeral materialization doesn't support model contracts. Materialized View The materialized_view materialization allows the creation and maintenance of materialized views in the target database. Materialized views are a combination of a view and a table, and serve use cases similar to incremental models. Pros: Materialized views combine the query performance of a table with the data freshness of a view Materialized views operate much like incremental materializations, however they are usually able to be refreshed without manual interference on a regular cadence (depending on the database), forgoing the regular dbt batch refresh required with incremental materializations dbt run on materialized views corresponds to a code deployment, just like views Cons: Due to the fact that materialized views are more complex database objects, database platforms tend to have fewer configuration options available; see your database platform's docs for more details Materialized views may not be supported by every database platform","title":"SQL Transformation and Modeling"},{"location":"dbt/sql_transformation_and_modeling_1/#materializations","text":"","title":"Materializations"},{"location":"dbt/sql_transformation_and_modeling_1/#overview","text":"Materializations are strategies for persisting dbt models in a warehouse. There are five types of materializations built into dbt. They are: Table View Incremental Ephemeral Materialized View","title":"Overview"},{"location":"dbt/sql_transformation_and_modeling_1/#table","text":"Pros: Table are fast to query. Cons: Tables can take a long time to rebuild, especially for complex transformations New records in underlying source data are not automatically added to the table","title":"Table"},{"location":"dbt/sql_transformation_and_modeling_1/#view","text":"When using the view materialization, your model is rebuilt as a view on each run, via a create view as statement. Pros: No additional data is stored, views on top of source data will always have the latest records in them. Cons: Views that perform a significant transformation, or are stacked on top of other views, are slow to query.","title":"View"},{"location":"dbt/sql_transformation_and_modeling_1/#incremental","text":"Pros: You can significantly reduce the build time by just transforming new records Cons: Incremental models require extra configuration and are an advanced usage of dbt","title":"Incremental"},{"location":"dbt/sql_transformation_and_modeling_1/#ephemeral","text":"Pros: You can still write reusable logic Ephemeral models can help keep your data warehouse clean by reducing clutter (also consider splitting your models across multiple schemas by using custom schemas). Cons: You cannot select directly from this model. Operations (for example, macros called using dbt run-operation cannot ref() ephemeral nodes) Overuse of ephemeral materialization can also make queries harder to debug. Ephemeral materialization doesn't support model contracts.","title":"Ephemeral"},{"location":"dbt/sql_transformation_and_modeling_1/#materialized-view","text":"The materialized_view materialization allows the creation and maintenance of materialized views in the target database. Materialized views are a combination of a view and a table, and serve use cases similar to incremental models. Pros: Materialized views combine the query performance of a table with the data freshness of a view Materialized views operate much like incremental materializations, however they are usually able to be refreshed without manual interference on a regular cadence (depending on the database), forgoing the regular dbt batch refresh required with incremental materializations dbt run on materialized views corresponds to a code deployment, just like views Cons: Due to the fact that materialized views are more complex database objects, database platforms tend to have fewer configuration options available; see your database platform's docs for more details Materialized views may not be supported by every database platform","title":"Materialized View"}]}